{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86820e77666d8529",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3\n",
    "## 1. Running Spark Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:36:18.751723Z",
     "start_time": "2024-05-10T16:35:39.917854Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib64/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa40619a5894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hive.metastore.uris\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"thrift://hadoop-02.uni.innopolis.ru:9883\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.warehouse.dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarehouse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.avro.compression.codec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"snappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1582\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1585\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1586\u001b[0m             answer, self._gateway_client, None, self._fqn)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team31\"\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"{} - spark ML\".format(team)) \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse) \\\n",
    "    .config(\"spark.sql.avro.compression.codec\", \"snappy\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baa5f6394fa59c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_name = f\"{team}_projectdb\"\n",
    "\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(f\"USE {db_name}\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d443746d5cdcae3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Read Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34732b110a600ed6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(spark.catalog.listTables(db_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f483f13d0ac57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mints = spark.read.format(\"avro\").table(f\"{db_name}.mints\").select(\"token_id\", \"timestamp\",\n",
    "                                                                   \"nft_address\",\n",
    "                                                                   \"transaction_value\")\n",
    "mints.printSchema()\n",
    "mints.show(5)\n",
    "mints.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473419cb240970c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfers = spark.read.format(\"avro\").table(f\"{db_name}.transfers\").select(\"token_id\", \"timestamp\",\n",
    "                                                                           \"transaction_value\")\n",
    "transfers.printSchema()\n",
    "transfers.show(5)\n",
    "transfers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c1a49e4e917b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfts = spark.read.format(\"avro\").table(f\"{db_name}.nfts\").select(\"address\", \"name\")\n",
    "nfts.printSchema()\n",
    "nfts.show(5)\n",
    "nfts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094c0915dcdb2ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. ML Modeling\n",
    "### 3.1 Feature Selection & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e011f0bb2cb2893",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "contract_stats = (\n",
    "    mints\n",
    "    .groupBy(\"nft_address\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"token_id\").alias(\"num_tokens\"),\n",
    "        F.avg(\"transaction_value\").alias(\"avg_mint_price\"),\n",
    "        F.max(\"transaction_value\").alias(\"max_mint_price\"),\n",
    "        F.min(\"transaction_value\").alias(\"min_mint_price\"),\n",
    "        F.min(\"timestamp\").alias(\"first_mint_date\"),\n",
    "        F.max(\"timestamp\").alias(\"last_mint_date\"),\n",
    "    )\n",
    ")\n",
    "contract_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ace0e3d2f3f713",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "def _get_last_n_txs(arr, n):\n",
    "    return arr[-n:]\n",
    "\n",
    "\n",
    "get_last_n_txs = F.udf(_get_last_n_txs, T.ArrayType(T.StructType([\n",
    "    T.StructField(\"timestamp\", T.LongType()),\n",
    "    T.StructField(\"transaction_value\", T.DoubleType()),\n",
    "])))\n",
    "\n",
    "LAST_N = 10\n",
    "\n",
    "nft_history = (\n",
    "    transfers\n",
    "    .groupBy(\"token_id\")\n",
    "    .agg(\n",
    "        F.sort_array(F.collect_list(F.struct(\"timestamp\", \"transaction_value\"))).alias(\"tx_data\"),\n",
    "    )\n",
    "    .withColumn(\"tx_data_except_last\", F.expr(\"slice(tx_data, 1, size(tx_data) - 1)\"))\n",
    "    .withColumn(\"last_tx\", F.element_at(\"tx_data\", -1))\n",
    "    .withColumn(\"last_tx_value\", F.col(\"last_tx.transaction_value\"))\n",
    "    .drop(\"last_tx\")\n",
    "    .drop(\"tx_data\")\n",
    "    # Overall stats (excluding last tx)\n",
    "    .withColumn(\"tx_count\", F.size(\"tx_data_except_last\"))\n",
    "    .withColumn(\"min_tx_value\", F.array_min(\"tx_data_except_last.transaction_value\"))\n",
    "    .withColumn(\"max_tx_value\", F.array_max(\"tx_data_except_last.transaction_value\"))\n",
    "    .withColumn(\"first_tx_timestamp\", F.element_at(\"tx_data_except_last.timestamp\", 1))\n",
    "    .withColumn(\"last_tx_timestamp\", F.element_at(\"tx_data_except_last.timestamp\", -1))\n",
    "    .withColumn(\"last_n_transactions\", get_last_n_txs(\"tx_data_except_last\", F.lit(LAST_N)))\n",
    "    .drop(\"tx_data_except_last\")\n",
    "    # Stats for the last N transactions\n",
    "    .withColumn(\"min_n_tx_value\", F.array_min(\"last_n_transactions.transaction_value\"))\n",
    "    .withColumn(\"max_n_tx_value\", F.array_max(\"last_n_transactions.transaction_value\"))\n",
    "    .withColumn(\"first_n_tx_timestamp\", F.element_at(\"last_n_transactions.timestamp\", 1))\n",
    "    .drop(\"last_n_transactions\")\n",
    ")\n",
    "nft_history.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81defac4e98db67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = (\n",
    "    mints.selectExpr(\"nft_address\", \"token_id\", \"timestamp as mint_timestamp\",\n",
    "                     \"transaction_value as mint_tx_value\")\n",
    "    .join(contract_stats, on=\"nft_address\", how=\"left\")\n",
    "    .join(nft_history, on=\"token_id\", how=\"left\")\n",
    "    .join(nfts, on=(mints.nft_address == nfts.address), how=\"left\")\n",
    "    .drop(\"nft_address\", \"address\")\n",
    ")\n",
    "print(features.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247155d5a1b76ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748dc7304ea74a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = features.withColumnRenamed(\"last_tx_value\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f38392079b6c98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_features = features.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8479e6fc30c6d2d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_features.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299f855af3b56fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3b9abd767121e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_cols = [\"mint_timestamp\", \"first_mint_date\", \"last_mint_date\", \"first_tx_timestamp\", \"last_tx_timestamp\", \"first_n_tx_timestamp\"]\n",
    "text_cols = [\"name\"]\n",
    "numerical_cols = [\"num_tokens\", \"avg_mint_price\", \"max_mint_price\", \"min_mint_price\", \"tx_count\", \"min_tx_value\", \"max_tx_value\", \"min_n_tx_value\", \"max_n_tx_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319e784f826f499",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_features_with_dt = filtered_features.selectExpr(\n",
    "    *(f\"from_unixtime({col}) as {col}\" if col in date_cols else col for col in\n",
    "      filtered_features.columns))\n",
    "filtered_features_with_dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2abbaeb3409516",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, Word2Vec, VectorAssembler, StandardScaler\n",
    "\n",
    "# Collection name encoding\n",
    "tokenizer = Tokenizer(inputCol=text_cols[0], outputCol=text_cols[0] + \"_tokens\")\n",
    "word2vec = Word2Vec(vectorSize=16, minCount=1, inputCol=tokenizer.getOutputCol(),\n",
    "                    outputCol=text_cols[0] + \"_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f2806b146a42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "\n",
    "# Date encoding\n",
    "class DateCyclicalEncodingTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable,\n",
    "                                      DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\",\n",
    "                      typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\",\n",
    "                       typeConverter=TypeConverters.toString)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(DateCyclicalEncodingTransformer, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "\n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "\n",
    "    def _transform(self, df):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        df = df.withColumn(output_col + \"_year\", F.year(F.col(input_col)))\n",
    "        for col, val_count in (\n",
    "                (\"month\", 12), (\"day\", 31), (\"hour\", 24), (\"minute\", 60), (\"second\", 60)):\n",
    "            df = (\n",
    "                df\n",
    "                .withColumn(output_col + f\"_{col}_sin\",\n",
    "                            F.sin(2 * math.pi * F.expr(f\"{col}({input_col})\") / val_count))\n",
    "                .withColumn(\n",
    "                    output_col + f\"_{col}_cos\",\n",
    "                    F.cos(2 * math.pi * F.expr(f\"{col}({input_col})\") / val_count)\n",
    "                )\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    def get_all_column_names(self):\n",
    "        output_col = self.get_output_col()\n",
    "        return [output_col + \"_year\"] + [output_col + f\"_{col}_sin\" for col in\n",
    "                                         (\"month\", \"day\", \"hour\", \"minute\", \"second\")] + [\n",
    "            output_col + f\"_{col}_cos\" for col in (\"month\", \"day\", \"hour\", \"minute\", \"second\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61991463deb6d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_transformers = [DateCyclicalEncodingTransformer(input_col=col, output_col=\"encoded_\" + col) for\n",
    "                     col in date_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3cb6b3cd1728c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_assemble = [text_cols[0] + \"_w2v\"] + sum(\n",
    "    (dt.get_all_column_names() for dt in date_transformers), []) + numerical_cols\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols_to_assemble, outputCol=\"raw_features\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce59dd64acb6706",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, word2vec] + date_transformers + [assembler, scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b043db454a1eb7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(filtered_features_with_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b23c11dee2d5d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_features = pipeline_model.transform(filtered_features_with_dt).select(\"features\",\n",
    "                                                                                  \"label\")\n",
    "transformed_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2468218a1ee5c04",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c828cf79b4b2ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = transformed_features.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc9195a476bfee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"json\") \\\n",
    "    .save(\"project/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d03e6ebf8581c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"json\") \\\n",
    "    .save(\"project/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a346eac30a612b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:23:04.734064Z",
     "start_time": "2024-05-10T16:22:59.963588Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `project/data/train/*.json': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat project/data/train/*.json > ../data/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8193dbafafd6c54e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:23:09.342067Z",
     "start_time": "2024-05-10T16:23:06.862827Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat project/data/test/*.json > ../data/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5da6f79334f6b5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:26:54.302356Z",
     "start_time": "2024-05-10T16:26:54.184262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gzip -c -9 ../data/train.json > ../data/train.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9260c8da8238ada4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T16:27:15.310009Z",
     "start_time": "2024-05-10T16:27:00.413802Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gzip -c -9 ../data/test.json > ../data/test.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6bcfb51fdd690",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b450a422b67c3abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:44.634915Z",
     "start_time": "2024-05-10T15:23:38.132047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "to_vector = F.udf(lambda vs: Vectors.dense(vs), VectorUDT())\n",
    "\n",
    "train_data = spark.read.json(\"project/data/train\").selectExpr(\"features.values as features\",\n",
    "                                                              \"label\").withColumn(\"features\",\n",
    "                                                                                  to_vector(\n",
    "                                                                                      \"features\"))\n",
    "test_data = spark.read.json(\"project/data/test\").selectExpr(\"features.values as features\",\n",
    "                                                            \"label\").withColumn(\"features\",\n",
    "                                                                                to_vector(\n",
    "                                                                                    \"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fb25df147631779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:44.640223Z",
     "start_time": "2024-05-10T15:23:44.636548Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "632d45f48b4c693c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:56.404850Z",
     "start_time": "2024-05-10T15:23:44.642048Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f489f32c42a3dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:56.412508Z",
     "start_time": "2024-05-10T15:23:56.406577Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 64\n",
      "RMSE: 1626087951446105856.00\n",
      "R2 score: 0.01945\n"
     ]
    }
   ],
   "source": [
    "training_summary = lr_model.summary\n",
    "print(\"Number of iterations:\", training_summary.totalIterations)\n",
    "print(f\"RMSE: {training_summary.rootMeanSquaredError:.2f}\")\n",
    "print(f\"R2 score: {training_summary.r2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd999bdcff0f298b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:56.543759Z",
     "start_time": "2024-05-10T15:23:56.413859Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|            features|        label|          prediction|\n",
      "+--------------------+-------------+--------------------+\n",
      "|[-4.7039155659731...|1.29999995E16|2.517081314455933...|\n",
      "|[-4.7039155659731...|          0.0|1.089464638468397...|\n",
      "|[-4.7039155659731...|5.50000009E17|6.157348480292416E16|\n",
      "|[-4.7039155659731...| 8.0000002E16|8.944542372326313...|\n",
      "|[-4.7039155659731...| 5.8799999E17|5.286698603387879...|\n",
      "+--------------------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_data)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e01a06626c926ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:23:56.558236Z",
     "start_time": "2024-05-10T15:23:56.545583Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                     metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca14ce3bca4913c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:24:01.757542Z",
     "start_time": "2024-05-10T15:23:56.559958Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1713754268755438592.00\n",
      "R2 score: 0.02456\n"
     ]
    }
   ],
   "source": [
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "r2_score = r2_evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2 score: {r2_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6eaa700c019a2a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:19.055005Z",
     "start_time": "2024-05-10T15:24:23.145022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_4ff1acd7785e, numFeatures=91"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(\n",
    "    lr_model.aggregationDepth, [2, 3, 4]) \\\n",
    "    .addGrid(lr_model.regParam, np.logspace(1e-3, 1e-1)\n",
    "             ) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=grid,\n",
    "                    evaluator=r2_evaluator,\n",
    "                    parallelism=5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c93ecb501031dcd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:19.064126Z",
     "start_time": "2024-05-10T15:27:19.056526Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_4ff1acd7785e', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 3,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='regParam', doc='regularization parameter (>= 0).'): 1.0747467053333597,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='LinearRegression_4ff1acd7785e', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(bestModel.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f40efeb4b6cdc535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:19.154820Z",
     "start_time": "2024-05-10T15:27:19.065464Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|            features|        label|          prediction|\n",
      "+--------------------+-------------+--------------------+\n",
      "|[-4.7039155659731...|1.29999995E16|2.516882016039310...|\n",
      "|[-4.7039155659731...|          0.0|1.089342662935894...|\n",
      "|[-4.7039155659731...|5.50000009E17|6.155812403158675...|\n",
      "|[-4.7039155659731...| 8.0000002E16|8.942848096075625...|\n",
      "|[-4.7039155659731...| 5.8799999E17|5.286557720543809...|\n",
      "+--------------------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "best_predictions = bestModel.transform(test_data)\n",
    "best_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4177d805e665a33d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:22.552195Z",
     "start_time": "2024-05-10T15:27:19.156240Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 1713754246732851200.00\n",
      "Best R2 score: 0.02456\n"
     ]
    }
   ],
   "source": [
    "best_rmse = rmse_evaluator.evaluate(best_predictions)\n",
    "best_r2_score = r2_evaluator.evaluate(best_predictions)\n",
    "print(f\"Best RMSE: {best_rmse:.2f}\")\n",
    "print(f\"Best R2 score: {best_r2_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3d2e9ab004fa42f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:22.557329Z",
     "start_time": "2024-05-10T15:27:22.553726Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 difference: 0.0000000251\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 difference: {best_r2_score - r2_score:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4268388954babd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:22.571473Z",
     "start_time": "2024-05-10T15:27:22.558970Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE difference: 22022587392.00\n",
      "RMSE improvement: 0.0000012850%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE difference: {rmse - best_rmse:.2f}\")\n",
    "print(f\"RMSE improvement: {(rmse - best_rmse) / rmse:.10%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "535597537cdebc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:46:25.535097Z",
     "start_time": "2024-05-10T15:46:25.530891Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE, Gwei: 1713754246.73285\n",
      "Best RMSE, ETH 1.71375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best RMSE, Gwei: {best_rmse / 10 ** 9:.5f}\")\n",
    "print(f\"Best RMSE, ETH {best_rmse / 10 ** 18:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79f5b87b4e83cd70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:23.834769Z",
     "start_time": "2024-05-10T15:27:22.573112Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = bestModel\n",
    "model1.write().overwrite().save(\"project/models/model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51e62a32c1c7d455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:25.665239Z",
     "start_time": "2024-05-10T15:27:23.836322Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -get project/models/model1 ../models/model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e4b917c6a89b1df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:37.689256Z",
     "start_time": "2024-05-10T15:27:25.666912Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_predictions.select(\"label\", \"prediction\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"project/output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7a24c333bf07fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:39.511153Z",
     "start_time": "2024-05-10T15:27:37.690715Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat project/output/model1_predictions.csv/*.csv > ../output/model1_predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2090bb21f7b15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5 Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53bc4a8f5575e799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:28:02.864004Z",
     "start_time": "2024-05-10T15:27:39.512704Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(seed=42)\n",
    "\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5a0dfd90201da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:28:03.053772Z",
     "start_time": "2024-05-10T15:28:02.865509Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|            features|        label|          prediction|\n",
      "+--------------------+-------------+--------------------+\n",
      "|[-4.7039155659731...|1.29999995E16|1.441254833912304...|\n",
      "|[-4.7039155659731...|          0.0|8.441977145012555...|\n",
      "|[-4.7039155659731...|5.50000009E17|1.445230285815225...|\n",
      "|[-4.7039155659731...| 8.0000002E16|1.444734375480371...|\n",
      "|[-4.7039155659731...| 5.8799999E17|3.863333462341892...|\n",
      "+--------------------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "predictions = gbt_model.transform(test_data)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e3bfab2d2bc8fab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:28:06.264493Z",
     "start_time": "2024-05-10T15:28:03.056965Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 863532489246270464.00\n",
      "R2 score: 0.75234\n"
     ]
    }
   ],
   "source": [
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "r2_score = r2_evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2 score: {r2_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a93cd0ff740bb70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:14.824947Z",
     "start_time": "2024-05-10T15:28:06.266023Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTRegressionModel: uid=GBTRegressor_0a672806dfda, numTrees=20, numFeatures=91"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = (grid.addGrid(gbt_model.maxDepth, [2, 5, 10, 15])\n",
    "        .addGrid(gbt_model.maxBins, [32, 128])\n",
    "        .build())\n",
    "\n",
    "cv = CrossValidator(estimator=gbt,\n",
    "                    estimatorParamMaps=grid,\n",
    "                    evaluator=r2_evaluator,\n",
    "                    parallelism=5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9240db04b2a458f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:14.834970Z",
     "start_time": "2024-05-10T15:45:14.826493Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='GBTRegressor_0a672806dfda', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'all',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'): 0.01,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 128,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='maxIter', doc='max number of iterations (>= 0).'): 20,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='seed', doc='random seed.'): 42,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
      " Param(parent='GBTRegressor_0a672806dfda', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'): 'squared'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(bestModel.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b605935c6b240b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:15.001678Z",
     "start_time": "2024-05-10T15:45:14.836208Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|            features|        label|          prediction|\n",
      "+--------------------+-------------+--------------------+\n",
      "|[-4.7039155659731...|1.29999995E16|1.592359448321518...|\n",
      "|[-4.7039155659731...|          0.0|1.365887221229401...|\n",
      "|[-4.7039155659731...|5.50000009E17|1.332288089284610...|\n",
      "|[-4.7039155659731...| 8.0000002E16|2.203297352529219...|\n",
      "|[-4.7039155659731...| 5.8799999E17|3.056335138725716...|\n",
      "+--------------------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "best_predictions = bestModel.transform(test_data)\n",
    "best_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a89cd10e1f12bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:18.793424Z",
     "start_time": "2024-05-10T15:45:15.003033Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 831145378236056576.00\n",
      "Best R2 score: 0.77057\n"
     ]
    }
   ],
   "source": [
    "best_rmse_2 = rmse_evaluator.evaluate(best_predictions)\n",
    "best_r2_score_2 = r2_evaluator.evaluate(best_predictions)\n",
    "print(f\"Best RMSE: {best_rmse_2:.2f}\")\n",
    "print(f\"Best R2 score: {best_r2_score_2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57e9e5c6aa328456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:18.797883Z",
     "start_time": "2024-05-10T15:45:18.794856Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 difference: 0.0182289766\n",
      "R2 improvement: 2.42298%\n"
     ]
    }
   ],
   "source": [
    "print(f\"R2 difference: {best_r2_score_2 - r2_score:.10f}\")\n",
    "print(f\"R2 improvement: {(best_r2_score_2 - r2_score) / r2_score:.5%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8bedef229a3970f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:18.815706Z",
     "start_time": "2024-05-10T15:45:18.799301Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE difference: 32387111010213888.00\n",
      "RMSE improvement: 3.7505376362%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE difference: {rmse - best_rmse_2:.2f}\")\n",
    "print(f\"RMSE improvement: {(rmse - best_rmse_2) / rmse:.10%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4feeb18938a7069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:39.338195Z",
     "start_time": "2024-05-10T15:45:39.323588Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE, Gwei: 831145378.23606\n",
      "Best RMSE, ETH 0.83115\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best RMSE, Gwei: {best_rmse_2 / 10 ** 9:.5f}\")\n",
    "print(f\"Best RMSE, ETH {best_rmse_2 / 10 ** 18:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e3de52f5d6790ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:21.009010Z",
     "start_time": "2024-05-10T15:45:18.817293Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = bestModel\n",
    "model2.write().overwrite().save(\"project/models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcb20da2b5426d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:23.022379Z",
     "start_time": "2024-05-10T15:45:21.010571Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -get project/models/model2 ../models/model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cad9591d38c36b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:37.423829Z",
     "start_time": "2024-05-10T15:45:23.024151Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "127358f870a73a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:45:39.298299Z",
     "start_time": "2024-05-10T15:45:37.426507Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat project/output/model2_predictions.csv/*.csv > ../output/model2_predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326ae1d02f98efe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.6 Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f13cab423c2e9337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:47:03.042827Z",
     "start_time": "2024-05-10T15:47:01.938360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1_predictions = spark.read.csv(\"project/output/model1_predictions.csv\", header=True, inferSchema=True)\n",
    "model2_predictions = spark.read.csv(\"project/output/model2_predictions.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55eaef42ae508bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:47:03.864848Z",
     "start_time": "2024-05-10T15:47:03.044406Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_rmse = rmse_evaluator.evaluate(model1_predictions)\n",
    "best_rmse_2 = rmse_evaluator.evaluate(model2_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7f3c27670d03593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:47:04.712744Z",
     "start_time": "2024-05-10T15:47:03.866629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_r2_score = r2_evaluator.evaluate(model1_predictions)\n",
    "best_r2_score_2 = r2_evaluator.evaluate(model2_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "806c5d6edd8533ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:47:08.084069Z",
     "start_time": "2024-05-10T15:47:04.715281Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+----------------------+--------------------+\n",
      "|model                                                                         |RMSE                  |R2                  |\n",
      "+------------------------------------------------------------------------------+----------------------+--------------------+\n",
      "|LinearRegressionModel: uid=LinearRegression_4ff1acd7785e, numFeatures=91      |1.71375424673285222E18|0.024561243833836843|\n",
      "|GBTRegressionModel: uid=GBTRegressor_0a672806dfda, numTrees=20, numFeatures=91|8.3114537823604992E17 |0.7705664592343757  |\n",
      "+------------------------------------------------------------------------------+----------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "models = [[str(model1), best_rmse, best_r2_score], [str(model2), best_rmse_2, best_r2_score_2]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Save it to HDFS\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c4ae08a79685ed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:47:14.550856Z",
     "start_time": "2024-05-10T15:47:12.620644Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat project/output/evaluation.csv/*.csv > ../output/evaluation.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
